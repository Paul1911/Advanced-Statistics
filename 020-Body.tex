\chapter{Workbook Assignment 1: Basic Probabilities and Visualisations}

\section{Bernoulli Distribution}
For each of the distributions below, please provide the requested graphics as well as the numeric results. In both cases, please provide how you realized these (calculations, code, steps…) and why it is the appropriate tools. Do not forget to include the scale of each graphics so a reader can read the numbers represented.

\section{Bernoulli Distribution}

Assumed a binary vote with outcome for or against is described by a Bernoulli distribution, with $P(vote = "for") = 0.69$. 
Interpreting this as a one-trial Bernoulli experiment with the two outcomes for and against - represented by success and no success respectively - we can visualize it as displayed in figure~\eqref{fig:1a}.


\begin{figure}[h]
\centering
\includegraphics[width=16cm]{pics/1a.pdf}
\caption{Bernoulli distribution of one trial}
\label{fig:1a}
\end{figure}
\FloatBarrier


As this is a one-trial Bernoulli experiment, we can only observe the two outcomes $success = for$ and $no~success = against$. With provided information that $P(vote = "for") = 0.69$, we can calculate that $P(vote=”against”) = 1 - P(vote = "for") = 0.31$. This is visualized by the two bars in figure~\eqref{fig:1a}. Following Hogg et al. \cite[Chapter~3.1]{hogg}, the expected value of X, described by a Bernoulli Distribution, is: 

\begin{equation}  \mu = E(X) = (0)(1-p) + (1)(p) = p
\label{eq:expvaluebernoulli}\end{equation}

Applied to our problem,  $ \mu = p = P(vote = "for") = 0.69 $.

This means if we would repeat the experiment one hundred times, we expect to observe 69 successes. 


This result was achieved using Python 3.9 and the following code:

\begin{minted}{python}
from scipy.stats import bernoulli
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

#Dataset creation. Done manually as only one trial
p_for = 0.69
p_against = 0.31 #p_against = 1-p_for
df = pd.DataFrame([[1,p_for],[0,p_against]])

#Visualisation
trace1 = go.Bar(x = df[0],y = df[1],width = 0.1,text = df[1], 
    textposition = 'auto')
fig = make_subplots(x_title = 'Number of Successes',y_title = 'P(X = x)')
fig.add_trace(trace1)
fig.show()
fig.write_image("figures/1a.pdf")
\end{minted}

\section{Poisson Distribution}

Assumed we try to model the number of meteorites falling on an ocean each year - why would a Poisson Distribution be a natural candidate?


The Poisson distribution is a discrete probability distribution representing the number of events occurring in a defined period. It assumes the events occur with a known constant mean rate and that the events are independent of the time since the last event \cite[Chapter~4.6]{illowsky2018introductory}.

The event of a meteor impacting the world in a given year fulfills all requirements for such distribution. The event itself is of discrete nature as you cannot have a fractional count of impacts, and we assume meteorite impacts occur with a known constant mean rate and the impacts themselves are independent of the time since the last event. We do not know, whether the variance of the distribution is equal to the mean, but even if it was, we could relax this assumption by using the Poisson-based Gamma-Poisson distribution. For this exercise, we assume $\sigma^2 = \mu$. Additionally assuming $\mu = 64$ allows an exemplary visualisation as in figure \eqref{fig:1b}.

\begin{figure}[h]
\centering
\includegraphics[width=16cm]{pics/1b.pdf}
\caption{Poisson distribution of annual meteorite impacts with $\mu = 64$}
\label{fig:1b}
\end{figure}
\FloatBarrier

To ensure readability of the graphic, probabilities $<0.5$ percent are excluded. Additionally, the variance ($s^2 = 64.36$) is visualised as its root, the sample standard deviation $s$ ($s = 8.02$). Looking at mean ($\bar{x} = 64.01$) and median ($m = 64$), one can barely see a difference, which is to be expected as the distribution displayed is not skewed. 

The calculations for this exercise were done in python 3.9 using Jupyter notebooks with the following code: 

\begin{minted}{python}
from scipy.stats import poisson
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

#creating a sample with fixed random state
size = 100000
raw = pd.Series(data = poisson.rvs(64, size=size, random_state = 42))

# calculating median, mean, variance, stdev
median = raw.median()
var = raw.var()  # sample assumed
mean = raw.mean()
stdev = np.sqrt(var)
stdev_range = pd.DataFrame([[raw.mean() - stdev, 0.025],
                            [raw.mean() + stdev, 0.025]])

# Filter to only display probabilities >=0.5%
df = pd.DataFrame(raw.value_counts())
df[0] = df[0].div(size)
df = df[df[0] >= 0.005]

# Display measures
print(var)
print(stdev)
print(mean)
print(median)

#creating graphic
trace1 = go.Bar(x = df.index, y = df[0],showlegend = False)
trace2 = go.Bar(x = [median], y = [0.055],width = [0.1], name = 'Median', 
text = [median], textposition = 'outside')
trace3 = go.Line(x = stdev_range[0], y = stdev_range[1], 
name = 'Standard Deviation', text = ['-1 s', '+1 s'],
mode ="lines+markers+text",textposition = "bottom center")
trace4 = go.Bar(x = [mean], y = [0.025],width = [0.15], name = 'Mean', 
text = [mean], textposition = 'outside')

fig = make_subplots(x_title = 'Annual Meteorite Impacts',
y_title = 'P(X = x)')
fig.add_trace(trace1)
fig.add_trace(trace2)
fig.add_trace(trace4)
fig.add_trace(trace3)
fig['layout'].update(barmode = 'overlay')
fig.show()

fig.write_image("figures/1b.pdf")
\end{minted}

\section{Exponential Distributions}
Assuming the probability that you need to wait more than the random variable of y hours to hear a owl from your room is described by:

\begin{equation}  f(Y > y) = 0.2\overline{8}\mathrm{e}^{-0.5y} + 0.7\overline{1}\mathrm{e}^{-0.25y}
\label{eq:initcdf}
\end{equation}
To facilitate calculations, we reformulate the periodic numbers to fractions:

\begin{equation}  f(Y > y) = \dfrac{13}{45}\mathrm{e}^{-0.5y} + \dfrac{32}{45}\mathrm{e}^{-0.25y}
\label{eq:initcdffractioned}
\end{equation}

Suspecting that such events are usually exponentially distributed, we reformulate the probability in equation \eqref{eq:truecdf} to the conventional expression \cite{deisenroth}. 

\begin{equation}  F_Y(y) = f(Y \leq y) =1-\left( \dfrac{13}{45}\mathrm{e}^{-0.5y} + \dfrac{32}{45}\mathrm{e}^{-0.25y} \right)
\label{eq:truecdf}
\end{equation}

As $Y$ and $y$ are defined as time and time cannot be negative in this case, the domain of the function we are interested in is $[0,+\infty]$.

\begin{equation} f'_Y(y) = f''(Y \leq y) = 
\dfrac{-8\mathrm{e}^{-\frac{y}{4}}-13\mathrm{e}^{-\frac{y}{2}}}{180}
\label{eq:secderivative}
\end{equation}

As \eqref{eq:secderivative}, the second derivative of formula \eqref{eq:truecdf},  additionally has no real-valued solution for $y=0$, we can describe $F_Y(y)$ of \eqref{eq:truecdf} as being upwards continuous monotonic increasing. Furthermore, $\lim_{y\to\infty} F_Y(y)= 1$ and $F_Y(0)= 0$ for the support of $y$. This is sufficient evidence to assume that the provided distribution of the random variable $y$ is a cumulative distribution function (CDF) of the family of exponential distributions. 

\begin{figure}[h]
\centering
\includegraphics[width=16cm]{pics/1c1.pdf}
\caption{Visualisation of provided and reformulated CDF of task assignment}
\label{fig:1c1}
\end{figure}
\FloatBarrier


For any calculation regarding the probability to hear an owl within a time interval, we use the CDF. If we for example wanted to know $P(2 \leq Y \leq 4)$, the probability of needing to wait two to four hours to hear an owl, we would calculate it as described in equation \eqref{eq:prob24}.

\begin{equation} P(2 \leq Y \leq 4) = P(Y \leq 4) - P(Y \leq 2)
\label{eq:prob24}
\end{equation}
Applied to our distribution, we calculate
\begin{equation} P (2 \leq Y \leq 4) =1-\left( \dfrac{13}{45}\mathrm{e}^{-2} + \dfrac{32}{45}\mathrm{e}^{-1}\right) -\left[ 1-\left( \dfrac{13}{45}\mathrm{e}^{-1} + \dfrac{32}{45}\mathrm{e}^{-0.5}\right)\right] 	\approx 0.23688696
\label{eq:probcalc24}
\end{equation}
Consequently, we can say that in 23.69 percent of cases, we have to wait in between two and four hours to hear an owl.
\\

Next to the CDF, we are also often interested in the corresponding probability density function (PDF). Generally, the PDF $f_Y(y)$ of a CDF $F_Y(y)$ is given by its first derivative \cite[Chapter~4.3]{montgomery2010applied}:
\begin{equation}
f_Y(y) = \frac{df(Y \leq y)}{dy}
\label{eq:firstderivative}
\end{equation}
For our CDF, the PDF is described by:
\begin{equation}
f_Y(y) = \dfrac{16\mathrm{e}^{-\frac{y}{4}}+13\mathrm{e}^{-\frac{y}{2}}}{90}
\label{eq:pdf}
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=17cm]{pics/1c2.pdf}
\caption{Visualisation of $f_Y(y)$ (PDF), the first two moments, and applicable quartiles}
\label{fig:1c2}
\end{figure}
\FloatBarrier

The graphical representation of the PDF in figure \eqref{fig:1c2} also includes the mean value. This is calculated using equation \eqref{eq:ExpValueHogg} following the definition of an expectation for a random variable in Hogg et al. \cite[Definition~1.8.1]{hogg}.
\begin{equation}
E(X) = \int^{+\infty}_{-\infty} x f_X(x) ~ dx
\label{eq:ExpValueHogg}
\end{equation}
Conditions for this to be true are that equation \eqref{eq:ExpValueHoggcondition} is fulfilled and that $X$ is a continuous random variable with $f_X(x)$ as its PDF.
\begin{equation}
\int^{+\infty}_{-\infty} |x| f_X(x) ~ dx < \infty
\label{eq:ExpValueHoggcondition}
\end{equation}
By the task assignment, we know that $y$ is a continuous random variable. By equation \eqref{eq:pdf}, we calculated $f_Y(y)$ as the PDF of $y$. Lastly, as $y \in [0,\infty)$, the left hand side of inequality \eqref{eq:ExpValueHoggcondition} reduces to the right hand side of equation \eqref{eq:ExpValueHogg} and the lower limit of the integral changes to 0. Consequently, if the mean or expected value $E(Y) < \infty$, it is valid. The calculation for our random variable $y$ in \eqref{eq:ExpValueCalculation} returns $E(Y) < \infty$, we can therefore accept $3.4\overline{2}$ hours as our expected value $E(Y)$ for the waiting time to hear an owl. 
%
\begin{equation}
E(Y) = \int^{+\infty}_{0} y f_Y(y) ~ dy = \int^{+\infty}_{0} y\left(\dfrac{16\mathrm{e}^{-\frac{y}{4}}+13\mathrm{e}^{-\frac{y}{2}}}{90} \right) ~dy = \frac{154}{45} = 3.4\overline{2}
\label{eq:ExpValueCalculation}
\end{equation}
Following Hogg et al., the first moment or mean $\mu = E(X)$, if $X$ is a random variable with existing expectation \cite[Definition~1.9.1]{hogg}. This is the case for $Y$. Consequently, we can also assume that the mean value $\mu = E(X) = 3.4\overline{2}$. 
\\

To calculate the second moment, the variance $\sigma^2$, we use definition 1.9.2 by Hogg et al. \cite{hogg} as equation \eqref{eq:HoggVariance}.
\begin{equation}
\sigma^2 = E(X^2) - \mu^2
\label{eq:HoggVariance}
\end{equation}
%
The variance of $y$ is calculated by \eqref{eq:YVariance}, using the same assumptions as for equation \eqref{eq:ExpValueCalculation}.
\begin{equation}
\sigma^2 = \int^{+\infty}_{0} y^2 f(y) ~ dy - \mu^2
= \int^{+\infty}_{0} \dfrac{y^2\left(16\mathrm{e}^{-\frac{y}{4}}+13\mathrm{e}^{-\frac{y}{2}}\right)}{90} - \left(\frac{154}{45}\right)^2
= \frac{27044}{2025} \approx 13.3551
\label{eq:YVariance}
\end{equation}

The variance is also depicted in figure \eqref{fig:1c2}     as its root, the standard deviation $\sigma \approx 3.6545$. 

Both figures \eqref{fig:1c1} and \eqref{fig:1c2} furthermore display the first three quartiles of the PDF. Hogg et al. \cite[Chapter~1.7.1]{hogg} define the second quartile, the median, as the point dividing area under the PDF into two areas of equal size, which again are divided into two areas of equal size by the first and third quartile. As the CDF is the integral of the PDF and consequently by nature describes the area under the curve of the PDF, we can simply use equation \eqref{eq:truecdf} for the calculation of quartiles. We know that each quartile has to cover a quarter of the area under the curve of PDF \eqref{eq:pdf}. This means we can substitute 0.25, 0.5 and 0.75 for $(F_Y \leq y)$ in equation \eqref{eq:truecdf} and solve for $y$ to get the quartiles. This is exemplarily done in equation \eqref{eq:firstquartile} for the first quartile.

\begin{equation}
\begin{split}
0.25 &
=1-\left( \dfrac{13}{45}\mathrm{e}^{-0.5y_1} + \dfrac{32}{45}\mathrm{e}^{-0.25y_1} \right)\\
y_1&=\ln \left(\frac{413529488+7788544\sqrt{2779}}{332150625}\right)\\
y_1 &\approx 0.9087 
\end{split}
\label{eq:firstquartile}
\end{equation}

The second and third quartile can be calculated in the same way, resulting in $y_2 \approx 2.2430$ for the second quartile and $y_3 \approx 4.6588$ for the third quartile. The calculations were done using symbolab \cite{symbolab} with the following commands:
\begin{lstlisting}
solve 0.25 = 1-\left( \dfrac{13}{45}\mathrm{e}^{-0.5y}+\dfrac{32}{45}
\mathrm{e}^{-0.25y} \right)

solve 0.5 = 1-\left( \dfrac{13}{45}\mathrm{e}^{-0.5y}+\dfrac{32}{45}
\mathrm{e}^{-0.25y} \right)

solve 0.75 = 1-\left( \dfrac{13}{45}\mathrm{e}^{-0.5y}+\dfrac{32}{45}
\mathrm{e}^{-0.25y} \right)
\end{lstlisting}

As $\lim_{y\to\infty} F_Y(y)= 1$, we would only get $\infty$ as a result if we tried to calculate the fourth quartile $y_4$. The first three quartiles are visualised in figure \eqref{fig:1c1} as points whereas in figure \eqref{fig:1c2}, they are displayed as vertical lines to indicate that we want to highlight that the individual quartile $y_i$ has $i*25$ percent of PDF \eqref{eq:pdf}'s area under the curve to its left. 

\begin{figure}[h]
\centering
\includegraphics[width=17cm]{pics/1c4.pdf}
\caption{Histogram of $f_Y(y)$ by minute}
\label{fig:1c4}
\end{figure}
\FloatBarrier

Lastly, figure \eqref{fig:1c4} displays the PDF as a histogram per minute for one observation. If we for example had 1'000 observations, we would hear an owl approximately in five observations in the first minute. As the PDF is per hour while the histogram should be per minute, we need to divide every value $f_Y(y_i)$ for our 600 data points by 60. The standard y-axis comprises 10 hours for all graphics, we have 600 columns which might be too many. However, the graphics are created with python and Plotly, which allows for magnification, if desired. 

Calculations and visualisations were done in Jupyter notebooks using the following python code:
\begin{minted}{python}
import numpy as np
import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots
import plotly
import plotly.express as px

# Define initial function as written in task assignment (P(Y>y))
def func(y):  # cdf
    return ((13 / 45) * np.exp(-0.5 * y) + (32 / 45) * np.exp(-0.25 * y))


# Reformulate function so that you have a standard exponential function (P(Y<=y)
def realfunc(y):  # cdf
    return 1 - func(y)


# Define pdf of realfunc(x)
def pdf_realfunc(y):
    # First derivative of realfunc(x) with respect to x
    return (16 * np.exp(-y / 4) + 13 * np.exp(-y / 2)) / (90)


# create datasets
func_data = []
realfunc_data = []
pdf_realfunc_data = []

for i in np.arange(0, 10, 1 / 60):
    func_data.append([i, func(i)])
    realfunc_data.append([i, realfunc(i)])
    pdf_realfunc_data.append([i, pdf_realfunc(i)])

# create dataframes
func_df = pd.DataFrame(func_data)
realfunc_df = pd.DataFrame(realfunc_data)
pdf_realfunc_df = pd.DataFrame(pdf_realfunc_data)

# Calculate mean value
y_mean = pdf_realfunc(154 / 45)

# Calculate quartiles
# symbolab.com: "solve 0.25 = 1-\left( \dfrac{13}{45}\mathrm{e}^{-0.5y}
# + \dfrac{32}{45}\mathrm{e}^{-0.25y} \right)"
q1 = np.log(413529488 / 332150625 + 7788544 * np.sqrt(2779) / 332150625)

# symbolab.com: "solve 0.5 = 1-\left( \dfrac{13}{45}\mathrm{e}^{-0.5y}
# + \dfrac{32}{45}\mathrm{e}^{-0.25y} \right)"
q2 = np.log(19342148 / 4100625 + 411904 * np.sqrt(2194) / 4100625)

# symbolab.com: "solve 0.75 = 1-\left( \dfrac{13}{45}\mathrm{e}^{-0.5y}
# + \dfrac{32}{45}\mathrm{e}^{-0.25y} \right)"
q3 = np.log(216370448 / 4100625 + 5392384 * np.sqrt(1609) / 4100625)

# Visualisation of initial function and reformulated function
color = px.colors.qualitative.Plotly
trace1 = go.Line(
    x=func_df[0], 
    y=func_df[1], 
    name='P(Y > y)'
)
trace2 = go.Line(
    x=realfunc_df[0], 
    y=realfunc_df[1], 
    name='P(Y <= y) (CDF)'
)
# Additionally, the PDF of P(Y <= y) can be displayed.
#trace3 = go.Line(x=pdf_realfunc_df[0],
#                y=pdf_realfunc_df[1],
#                name='PDF of P(Y <= y)'
#)
trace9 = go.Scatter(
    x=[q1],
    y=[0.25],
    showlegend=True,
    name='Quartiles 1-3',
    marker_color=color[9],
    hovertext=['1st Quartile'],
)
trace10 = go.Scatter(
    x=[q2],
    y=[0.5],
    showlegend=False,
    marker_color=color[9],
    hovertext=['2nd Quartile (Median)'],
)
trace11 = go.Scatter(
    x=[q3],
    y=[0.75],
    showlegend=False,
    marker_color=color[9],
    hovertext=['3rd Quartile'],
)
# Mean (can also be displayed additionally)
#trace6 = go.Scatter(x=[154 / 45], y=[y_mean], name='Mean')

# Creating graphic
fig = make_subplots(x_title="y (Hours)", y_title="Probability")
fig.add_trace(trace1)
fig.add_trace(trace2)
#fig.add_trace(trace3)
#fig.add_trace(trace6)
fig.add_trace(trace9)
fig.add_trace(trace10)
fig.add_trace(trace11)
fig.show()
fig.write_image("figures/1c1.pdf")

# Visualisation of PDF
color = px.colors.qualitative.Plotly
trace3 = go.Line(x=pdf_realfunc_df[0],
                 y=pdf_realfunc_df[1],
                 name='f_Y(y) (PDF)',
                 marker_color=color[0])
trace6 = go.Scatter(x=[154 / 45],
                    y=[y_mean],
                    showlegend=True,
                    name='Mean',
                    marker_color=color[2])
trace7 = go.Bar(x=[154 / 45],
                y=[y_mean],
                showlegend=True,
                name='Mean',
                marker_color=color[2],
                width=0.05)
trace8 = go.Line(x=[154 / 45, 154 / 45 + 3.66],
                 y=[y_mean, y_mean],
                 name='Standard Deviation',
                 text=['Mean', '+1 std. dev.'],
                 mode="lines+markers+text",
                 textposition="top center",
                 fillcolor=color[3])
trace9 = go.Bar(
    x=[q1],
    y=[pdf_realfunc(q1)],
    showlegend=True,
    name='Quartiles 1-3',
    marker_color=color[9],
    width=0.05,
    text=['1st Quartile'],
    textposition='outside',
)
trace10 = go.Bar(
    x=[q2],
    y=[pdf_realfunc(q2)],
    showlegend=False,
    marker_color=color[9],
    width=0.05,
    text=['2nd Quartile (Median)'],
    textposition='outside',
)
trace11 = go.Bar(
    x=[q3],
    y=[pdf_realfunc(q3)],
    showlegend=False,
    marker_color=color[9],
    width=0.05,
    text=['3rd Quartile'],
    textposition='outside',
)

fig = make_subplots(x_title="y (Hours)", y_title="P(y)")
fig.add_trace(trace3)
fig.add_trace(trace8)
fig.add_trace(trace6)
#fig.add_trace(trace7) #Additionally, this line can be uncommented
#to get a vertical line for the mean for better visibility. This
#might however be confusing as the mean is no indicator for area
#under the curve, contrary to quartiles, which are displayed as
#columns. Thus, this column was left in as comment, in case some-
#one wants better visuability
fig.add_trace(trace9)
fig.add_trace(trace10)
fig.add_trace(trace11)

fig.update_layout(bargap=0.0, barmode='overlay')
fig.update_traces(marker_line_width=0)

fig.show()
fig.write_image("figures/1c2.pdf")

# Zooming in on the mean, if wanted
#fig.update_xaxes(range=[3.37, 3.47])
#fig.update_yaxes(range=[0.099, 0.105])
#fig.show()
#fig.write_image("figures/1c3.pdf")

#Creating the Histogram per minute

trace4 = go.Bar(y=pdf_realfunc_df[1] / 60,
                name="Histogram of f_Y(y)",
                marker_color='#3399FF')

fig = make_subplots(x_title="y", y_title="P(y)")

fig.add_trace(trace4)

fig.update_layout(bargap=0.0, barmode='overlay')
fig.update_traces(marker_line_width=0)

fig.show()
fig.write_image("figures/1c4.pdf")
\end{minted}



\chapter{Workbook Assignment 2: Basic Probabilities and Visualizations }	

\section{Visualisation, Expectations and (Co)Variances}
Assumed you have recorded mappings of X and Y values, what is an appropriate way of visualisation and how do you find the sample covariance as well as the expectation values and variances per variable; assuming the data comes in the following format?\\
\\
(1.754, 720.15), (-7.385, -260.86), (1.396, -340.56), (-3.304, 954.75), 
(-12.159, -370.12),\\ (-6.767, -259.89), (1.233, 261.89), (-7.056, 400.72), 
(-1.222, -94.33), (-0.722, -492.02),\\ (-5.065, -789.42), (-9.333, 301.86), 
(7.27, -17.04), (-1.989, 257.91), (3.582, -229.89), \\(-1.2, -349.18), 
(4.118, 226.05), (-1.834, -534.54), (-1.882, 610.91), (8.677, 70.11) \\

As the variables in the data are both continuous and the question addresses the calculation of correlation, the optimal visualisation is a scatter diagram as for example figure~\eqref{fig:WorkbookAssignment2a}.

\begin{figure}[h]
\centering
\includegraphics[width=16cm]{pics/WorkbookAssignment2a.pdf}
\caption{Visualisation of provided X and Y values as scatter plot}
\label{fig:WorkbookAssignment2a}
\end{figure}
\FloatBarrier

Looking at the scatter plot, one can observe that the data is quite evenly dispersed; No pattern can be determined at first glance. This rather evenly distributed data also suggests that the expected value for X and Y is rather close to 0. Calculating the expected value for each variable can be understood at the calculation of the expected value of a discrete random variable as the number of observations is discrete. Equation~\eqref{eq:ExpValDiscrete} is therefore the formula to calculate the expected value. 


\begin{equation}  E(X) = \sum [x * P(x)]
\label{eq:ExpValDiscrete}
\end{equation}

http://mathcenter.oxford.emory.edu/site/math117/expectedValueOfADiscreteRandomVariable/
\\

The results are consequently $E(X) = -1.5944$ and $E(Y) = 3.3250$.

The sample covariance is defined in Equation~\eqref{eq:COV}, where $\bar{x} = E(X)$ and $\bar{y} = E(Y)$ (adapted from~\cite{bruce2017practical}).

\begin{equation}  s_{xy} = \frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}
\label{eq:COV}
\end{equation}

Applying the formula to the provided data, $s_{xy} = 284.23$ can be observed. This indicates a positive relationship.

The calculations were done using Python 3.9 and the following code:

\begin{minted}{python}
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
data = (1.754, 720.15), (-7.385, -260.86), (1.396, -340.56), (-3.304, 954.75),
(-12.159, -370.12), (-6.767, -259.89), (1.233, 261.89), (-7.056, 400.72), 
(-1.222, -94.33), (-0.722, -492.02), (-5.065, -789.42), (-9.333, 301.86),
(7.27, -17.04), (-1.989, 257.91), (3.582, -229.89), (-1.2, -349.18), 
(4.118, 226.05), (-1.834, -534.54), (-1.882, 610.91), (8.677, 70.11)
data = pd.DataFrame(data)
data.columns = ["X", "Y"]
print("The variance of the variables is: \n{}".format(data.var()))
print("The expectation value for the variables is: \n{}".format(data.mean()))
print("The sample covariance is {}".format(data.cov().iloc[0,1]))

data = data.set_index("X")

sns.scatterplot(data = data, x = "X", y = "Y", color = "black").set(
    title = "Visualisation of X and Y Values")
plt.savefig('./figures/2a.pdf')
\end{minted}


\section{Transformations}
Assumed we have a ball being thrown at a random angle $\theta$ [0,360) (in degrees) and a random radius $r$ $\in$ [0,1] (in meters) both independent and uniform. What is the density of the variables $X$ and $Y$, which are the Cartesian coordinates of the point at angle $\theta$ and radius $r$? And, furthermore, what is their expectation and variance?

Following Illowsky and Dean \cite[Chapter~5.2]{illowsky2018introductory}, the pdf of an uniformly distributed variable is $f(x) = \frac{1}{b-a}$ for $a \leq x \leq b$. 

As $\theta,r \sim \mathcal{U}$, we can formulate the marginal distributions as described in \eqref{eq:theta_untransformed} (using radiants) and \eqref{eq:r_untransformed}.

\begin{equation}
f_\Theta(\theta) = 
\begin{cases} 
      \frac{1}{2\pi} & for~\theta \in [0,2\pi)\\
      0&elsewhere
   \end{cases}
\label{eq:theta_untransformed}
\end{equation}

\begin{equation}
f_R(r) = 
\begin{cases} 
      1 & for~r\in[0,1]\\
      0&elsewhere
   \end{cases}
\label{eq:r_untransformed}
\end{equation}

Joining these distributions returns the following joint pdf:
\begin{equation}
\begin{split}
f_{R,\Theta}(r,\theta) = 
\begin{cases} 
      \frac{1}{2\pi} & \theta \in [0,2\pi),~ r\in[0,1]\\
      0&elsewhere
   \end{cases}
\end{split}
\label{eq:joint_pdf_untransformed}
\end{equation}

Following Dawkins \cite{dawkins}, we can transform to Cartesian $x$ and $y$ using $x = rcos(\theta)$ and $y=rsin(\theta)$. For the transformation, we furthermore need their inverses, which calculate to 

\begin{align*}
x^2=r^2cos^2(\theta)& \qquad y^2=r^2sin^2(\theta)\\
x^2+y^2&=r^2cos^2(\theta)sin^2(\theta)\\
x^2+y^2&=r^2\\
r &= \sqrt{x^2+y^2} = u_1(x,y)\\
\label{eq:r^-1}
\end{align*}
and
\begin{align*}
 r = \frac{x}{cos(\theta)}& \qquad r = \frac{y}{sin(\theta)}\\
 \frac{x}{cos(\theta)} &= \frac{y}{sin(\theta)}\\
 \frac{x}{cos(\theta)} &= \frac{y}{sin(\theta)}\\
 \frac{y}{x} &= \frac{cos(\theta)}{sin(\theta)}\\
 \frac{y}{x} &= tan(\theta)\\
 \theta &= tan^{-1}(\frac{y}{x}) = u_2(x,y)
\label{eq:theta^-1}
\end{align*}

Using the transformation theorem proved in Hogg et al. \cite[Chapter~2.7]{hogg}, we get
 
\begin{equation}
\begin{split}
f_{X,Y}(x,y) = f_{R,\Theta}\left(\sqrt{x^2+y^2},tan^{-1}\left(\frac{y}{x}\right)\right) \cdot |J|
%f_{X,Y}(x,y) = f_{r,\theta}\left(u_1,u_2\right)\right) \cdot |J|
\end{split}
\label{eq:transtheorem}
\end{equation}
where 
\begin{equation}
\begin{split}
|J| &=
\left|
\begin{matrix}
\frac{\partial u_1}{\partial x} & 
\frac{\partial u_1}{\partial y}  \\[2ex] % <-- 1ex more space between rows of matrix
\frac{\partial u_2}{\partial x} & 
\frac{\partial u_2}{\partial y}  \\[2ex]
\end{matrix}
\right|
= 
\left|
\begin{matrix}
\frac{x}{\sqrt{x^2+y^2}}& 
\frac{y}{\sqrt{x^2+y^2}}  \\[2ex] % <-- 1ex more space between rows of matrix
-\frac{y}{y^2+x^2} & 
\frac{x}{y^2+x^2}  \\[2ex]
\end{matrix}
\right|\\[2ex]
&= \frac{x}{\sqrt{x^2+y^2}} \cdot \frac{x}{y^2+x^2} -\frac{y}{\sqrt{x^2+y^2}}  \cdot \left(-\frac{y}{y^2+x^2}\right)\\[2ex]
&= \frac{x^2+y^2}{(x^2+y^2)^{3/2}}\\[2ex]
&=\frac{1}{\sqrt{x^2+y^2}}
\end{split}
\label{eq:jacobiandet}
\end{equation}

with the derivatives being calculated by Symbolab \cite{symbolab} with the following commands:
\begin{lstlisting}
\frac{\partial }{\partial \:x}\left(\sqrt{x^2+y^2}\right)
\frac{\partial }{\partial \:y}\left(\sqrt{x^2+y^2}\right)
\frac{\partial }{\partial \:x}\left(tan^{-1}\left(\frac{y}{x}\right)\right)
\frac{\partial }{\partial \:y}\left(tan^{-1}\left(\frac{y}{x}\right)\right)
\end{lstlisting}

Substituting \eqref{eq:jacobiandet} into \eqref{eq:transtheorem}, we get
\begin{equation}
\begin{split}
f_{X,Y}(x,y) &= f_{r,\theta}\left(\sqrt{x^2+y^2},tan^{-1}\left(\frac{y}{x}\right)\right) \cdot \frac{1}{\sqrt{x^2+y^2}}\\[2ex]
f_{X,Y}(x,y) &= \frac{1}{2\pi} \cdot \frac{1}{\sqrt{x^2+y^2}} = \frac{1}{2\pi\sqrt{x^2+y^2}}
%\begin{cases} 
%      \frac{1}{2\pi} & \theta \in [0,2\pi),~ r\in[0,1]\\
%      0&elsewhere
%   \end{cases}
\end{split}
\label{eq:transtheoremcomplete}
\end{equation}

which represents the joint PDF of $X$ and $Y$.\\
To calculate their individual densities, the marginal distributions are needed. As proofed by Larsen and Marx \cite[Theorem~3.7.2]{larsen2005introduction}, the marginal distribution for $X$ can be obtained by 

\begin{equation}
\begin{split}
f_{X}(x) = \int_{0}^{\sqrt{1-x^2}}\frac{1}{2\pi\sqrt{x^2+y^2}} dy
\end{split}
\label{eq:marginalX}
\end{equation}

which calculates to 

\begin{equation}
\begin{split}
f_{X}(x) = \frac{1}{2\pi }\ln \left(\frac{\sqrt{1-x^2}+1}{x}\right) \quad x \in (0,1)
%https://www.symbolab.com/solver/step-by-step/%5Cint_%7B0%7D%5E%7B%5Csqrt%7B1-x%5E2%7D%7D%5Cfrac%7B1%7D%7B2%5Cpi%5Csqrt%7Bx%5E%7B2%7D%2By%5E%7B2%7D%7D%7D%20dy
\end{split}
\label{eq:marginalXcalculated}
\end{equation}

using Symbolab \cite{symbolab} with the following code:
\begin{lstlisting}
\int _0^{\sqrt{1-x^2}}\frac{1}{2\pi \sqrt{x^2+y^2}}\:dy
\end{lstlisting}

PDF \eqref{eq:transtheoremcomplete} is symmetric in $x$ and $y$ as $f_{X,Y}(x,y) = f_{Y,X}(y,x)$. Consequently, $f_Y(y)$ has similar form:
\begin{equation}
\begin{split}
f_{Y}(y) = \frac{1}{2\pi }\ln\left(\frac{\sqrt{1-y^2}+1}{y}\right) \quad y \in (0,1)
\end{split}
\label{eq:marginalYcalculated}
\end{equation}
To calculate the expected value, equation \eqref{eq:ExpValueHogg} is used again:
\begin{equation}
\begin{split}
E(X) = \int_0^1  \frac{x}{2\pi }\ln \left(\frac{\sqrt{1-x^2}+1}{x}\right) dx\\
E(X) = \frac{1}{2\pi}\int_0^1  x\ln \left(\frac{\sqrt{1-x^2}+1}{x}\right) dx\\
E(X) = \frac{1}{4\pi } \cdot \left[x^2\ln \left(\frac{\sqrt{1-x^2}+1}{x}\right)-\sqrt{1-x^2}\right]_0^1\\
E(X) = E(Y) = \frac{1}{4\pi} \approx 0.07958
\end{split}
\label{eq:expvalF_X}
\end{equation}
as again the same value calculates for $Y$ due to the symmetry of the distribution.
WolframAlpha \cite{wolfalph} was used for this calculation with the following input:
\begin{lstlisting}
\int_0^1  \frac{x}{2\pi }\ln \left(\frac{\sqrt{1-x^2}+1}{x}\right)
\end{lstlisting}

To calculate the second moment, the variance, equation \eqref{eq:HoggVariance} is recalled:
\[
Var(x) = E(X^2)-(E(X))^2
\]
where
\begin{equation}
\begin{split}
E(X^2) =  \frac{1}{2\pi}\int_0^1  x^2\ln \left(\frac{\sqrt{1-x^2}+1}{x}\right) dx = \frac{1}{24}
\end{split}
\label{eq:E(X^2)}
\end{equation}
This calculation was done in WolframAlpha \cite{wolfalph} with the following input:
\begin{lstlisting}
\int_0^1  \frac{x^2}{2\pi }\ln \left(\frac{\sqrt{1-x^2}+1}{x}\right)
\end{lstlisting}
Consequently, 
\begin{equation}
\begin{split}
Var(X) = Var(Y) = \frac{1}{24}- \left(\frac{1}{4\pi}\right)^2 = \frac{2\pi ^2-3}{48\pi ^2} \approx 0.1581
\end{split}
\label{eq:VarXcalculated}
\end{equation}



\chapter{Workbook Assignment 3: A Simple Parameter Estimation}	

Supposed we have a network router with a total bandwith until it breaks down described by the random variable $S$, which is exponentially distributed:
\begin{equation}
\begin{split}
%s(t) = f_S(t) =  \frac{1}{\theta} e^{-\frac{t}{\theta}}
s(t) =  \frac{1}{\theta} e^{-\frac{t}{\theta}}
\end{split}
\label{eq:exphardwarefailure}
\end{equation}

\section{Bandwith Total to Failure}
Assumed we have another backup device, the bandwidth total to failure of both devices is described by $T$, which is the sum of $S_1$ and $S_2$, which each are described by \eqref{eq:exphardwarefailure}. T is consequently
\begin{equation}
\begin{split}
T = S_1 + S_2
\end{split}
\label{eq:T}
\end{equation}
Both variables are random variables by task assignment and it can furthermore be assumed that they are independent as the performance of one device has no influence on the performance of another device. Furthermore, bandwidth is a continuous variable that cannot take negative values. With these assumptions about the variables ,the sum of both variables is consequently calculated by convolution, following Larsen and Marx \cite[Theorem~3.8.3]{larsen2005introduction}.
\begin{equation}
\begin{split}
t(z) = \int_{-\infty}^\infty s(t)s(z-t)~dt
\end{split}
\label{eq:tz1}
\end{equation}
As only positive values can be obtained for the bandwidth, \eqref{eq:tz1} simplifies to 
\begin{equation}
\begin{split}
t(z) = \int_{0}^zs(t)s(z-t)~dt
\end{split}
\label{eq:tz2}
\end{equation}
inserting the density of $S$ yields
\begin{equation}
\begin{split}
t(z) 
&= \int_{0}^z \left(\frac{1}{\theta} e^{-\frac{t}{\theta}}\right)
\left(\frac{1}{\theta} e^{-\frac{z-t}{\theta}}\right)~dt\\
&= \frac{1}{\theta^2} e^{-\frac{z}{\theta}}\int_{0}^z~dt\\
&= \frac{z}{\theta^2} e^{-\frac{z}{\theta}}\\
&= f_T(z;~\theta)
\end{split}
\label{eq:tz3}
\end{equation}

\section{Likelihood Function}
Given an experiment with the dual-router-system yielding a sample $T_1$, $T_2$, ..., $T_n$, calculate the likelihood function for $\theta$. Propose a transformation of this likelihood function whose maximum is the same and can be computed easily.

Following Definition 5.2.1 in Larsen and Marx \cite{larsen2005introduction}, the likelihood function is described by
\begin{equation}
\begin{split}
L(\theta) = \prod_{i=1}^n f_Y(y_i;~\theta)
\end{split}
\label{eq:LikelyhoodFunctionDefinition}
\end{equation}
which, in the case of this tasks, is 
\begin{equation}
\begin{split}
L(\theta) 
&= \prod_{i=1}^n f_T(z_i;~\theta)\\
&= \prod_{i=1}^n \frac{z_i}{\theta^2} e^{-\frac{z_i}{\theta}}\\
\end{split}
\label{eq:LikelyhoodFunction2}
\end{equation}
and simplifies to 
\begin{equation}
\begin{split}
L(\theta) 
&= \theta^{-2n}\left(\prod_{i=1}^n z_i\right)e^{-\frac{1}{\theta}\sum_{i=1}^n z_i}
\end{split}
\label{eq:LikelyhoodFunction3}
\end{equation}
To calculate the maximum likelihood, equation \eqref{eq:LikelyhoodFunction3} must be derived with respect to $\theta$ and equated to $0$. In its current form, such calculation quickly becomes computationally expensive and eventually inaccurate. To surpass such problem, \eqref{eq:LikelyhoodFunction3} can be transformed using the natural logarithm as the value maximising $L(\theta)$ also maximises $\ln L(\theta)$:
\begin{equation}
\begin{split}
\ln~L(\theta) 
&= -2n\ln\theta + \ln\left(\prod_{i=1}^n z_i\right)-\frac{1}{\theta}\sum_{i=1}^n z_i
\end{split}
\label{eq:LikelyhoodFunction4}
\end{equation}
The derivation then yields
\begin{equation}
\begin{split}
\frac{d\ln~L(\theta)}{d\theta}
&= \frac{-2n}{\theta}+\frac{1}{\theta^2}\sum_{i=1}^n z_i
\end{split}
\label{eq:LikelyhoodFunction5}
\end{equation}
which results in the maximum likelihood function when equated to $0$ and solved for $\theta$:
\begin{equation}
\begin{split}
\frac{-2n}{\theta}+\frac{1}{\theta^2}\sum_{i=1}^n z_i = 0
\end{split}
\label{eq:LikelyhoodFunction6}
\end{equation}
The maximum likelihood estimator $\theta_e$ is consequently 
\begin{equation}
\begin{split}
\theta_e =\frac{1}{2n}\sum_{i=1}^n z_i
\end{split}
\label{eq:MLE}
\end{equation}
In an experiment with $n=5$ observations measuring the total bandwidth to failure, the following results were observed:
\begin{center}
16, 86, 96, 17, 339
\end{center}
To estimate the model-parameter with the maximum likelihood, the values are inserted into \eqref{eq:MLE}:
\begin{equation}
\begin{split}
\theta_e =\frac{1}{2\cdot5}(16+86+96+17+339) = 55.4
\end{split}
\label{eq:MLECalculation}
\end{equation}
To calculate the expectation of the bandwidth, we reuse equation \eqref{eq:ExpValueHogg} which, applied to this task, is
\begin{equation}
\begin{split}
E(T) = \int_0^\infty z\frac{z}{\theta^2} e^{-\frac{z}{\theta}}~dz\\
\end{split}
\label{eq:expvalue3}
\end{equation}
and calculates to 
\begin{equation}
\begin{split}
E(T) 
&= \int_0^\infty z\frac{z}{55.4^2} e^{-\frac{z}{55.4}}~dz\\
&= 110.8
\end{split}
\label{eq:expvalue4}
\end{equation}
Equation \eqref{eq:expvalue4} was calculated using symbolab \cite{symbolab} with the following command:
\begin{lstlisting}
\int _0^{\infty }\:z\frac{z}{55.4^2}\:e^{-\frac{z}{55.4}}
\end{lstlisting}

\chapter{Workbook Assignment 4: Hypothesis Test}	

Assumed we are producing high-quality hammers in our factory and want to model the production quality. From the last 1000 produced hammers, we observed an average weight of $\mu_1=971~g$ and a standard deviation of $\sigma_1 = 42~g$. As $n = 1000$, we assume that we can treat the point estimates for the mean and the standard deviation as population parameters.

Production inaccuracies are usually normally dispersed, and as we neither have access to the raw data of individual observations nor have information on any skewness or non-normal behaviour, a normal distribution $N(\mu, \sigma^2)$ is assumed.
Hogg et al. \cite[Definition~3.4.1]{hogg} define the normal distribution as follows: 

\begin{equation} \frac{1}{{\sigma \sqrt {2\pi } }}e^{{{ - \left( {x - \mu } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - \mu } \right)^2 } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace}{2\sigma ^2 }}}
\label{eq:normaldist}
\end{equation}


It depends on two parameters, the population standard deviation $\sigma$ and the population mean $\mu$, which both are provided. In general, any normal distribution has a bell-shaped form, indicating that it is symmetric and not skewed to any side. Apart from the mean and the standard deviation, we are not provided any further data.  The assumption of such model of course does not hold anymore if the data of the 1000 produced hammers introduces any kind of bias or skewness. 


Other distributions are excluded as they are not applicable to this case or are less fitting. A Binomial Distribution for example is not applicable as it is binary, whereas the independent variable central to this exercise - hammer weight - is continuous. \\Describing the hammer weight as a repeated Bernoulli trial would also be an option to model the distribution, however such endeavor would firstly approximate a normal distribution as well and would secondly assume that the weight measurements are integer variables, whereas they are of continuous nature. \\
Another popular distribution, the Poisson distribution, is also not applicable as it models the average number of events per time interval \cite{bruce2017practical}. \\
Using an exponential distribution to model the weight dispersion of hammer production would imply that we expect for example continuously increasing counts of hammers for increasing weights, which is not a senseful assumption. \\
Consequently, we assume that the normal distribution is the best approximation.

Using the normal distribution as approximation implies accepting several assumptions. We assume that the model has the x-axis as an horizontal asymptote, that its mean is equal to its median, that its mode is the same as its mean and that it is symmetric about the mean \cite[Chapter~3.4]{hogg}.

Using \eqref{eq:normaldist} with the provided parameters of $\mu_1 = 971$ and $\sigma_1 = 42$, the following model is proposed: 

\begin{equation} \frac{1}{{42 \sqrt {2\pi } }}e^{{{ - \left( {x - 971 } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - 971 } \right)^2 } {2(42) ^2 }}} \right. \kern-\nulldelimiterspace}{2(42) ^2 }}} ~~~=~~~ \frac{1}{{42 \sqrt {2\pi } }}e^{{{ - \left( {x - 971 } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {x - 971 } \right)^2 } {7056 }}} \right. \kern-\nulldelimiterspace}{3528 }}}
\label{eq:normaldistadapted}
\end{equation}



Assumed a new production system for hammer production is tested. A random sample output of hammers of this new production output returns the following weights in grams: \\

\begin{center}
    1009, 1069, 966, 952, 1084, 897, 909, 995, 975, 983
\end{center}

How do we know if this new system's weight accuracy is better than the old system's accuracy, i.e. how do we find out whether the weight variance of the new system is lower than the variance of the old system?

To answer this question, we have to perform a statistical test comparing the variances of both samples, which requires hypotheses. Overall, we expect that the new system reduces weight variance and is more accurate. The hypotheses are consequently 

\begin{center}
$H_0~~:~~ \sigma^2_2~=~\sigma^2_1$\\ 
$H_1~~:~~ \sigma^2_2~<~\sigma^2_1$
\end{center}

where $\sigma^2_1$ is defined as the population variance of the old production system and $\sigma^2_2$ is defined as the population variance of the new production system.

Following Larsen and Marx \cite[Chapter 7.5]{larsen2005introduction}, the chi square distribution is needed for inferences on variances. To test our sample variance $s_2^2 = \frac{32594.9}{9} \approx 3621.66 $ against the variance $\sigma^2_1 = 42^2 = 1764$, we can now use the Chi Square Test or the F-Test. As we earlier assumed that $\sigma_1$ is the population variance instead of a sample variance, we do not need to rely on F-Test and can work with the Chi Square Test. The hypotheses can then be formulated as 

\begin{center}
$H_0~~:~~ \sigma^2_2~=~1764$\\ 
$H_1~~:~~ \sigma^2_2~<~1764$
\end{center}

Following Larsen and Marx \cite[Chapter 7.5]{larsen2005introduction}, the Chi Square test statistic is given by 

\begin{equation}
\chi^2 = \frac{(n-1)s_2^2}{\sigma_1^2}
\label{eq:chisquarestatistic}
\end{equation}

with the decision rule of rejecting $H_0$ if 
\\
\begin{equation}
\chi^2 \leq \chi^2_{1-\alpha,n-1}
\label{eq:decisionrule}
\end{equation}
\\
where our error probability $\alpha$ is our level of significance. An usual assumption is a 95 percent confidence interval, which results in $\alpha = 0.05$. As the losses which an error in this test might incur can be expected to be quite minor, there is no need to increase the confidence interval and to work with a smaller value of $\alpha$.


Inserting the sample figures into \eqref{eq:chisquarestatistic}, we observe 
\begin{equation}
\chi^2 = \frac{(10-1) \cdot \frac{32594.9}{9}}{1764} = \frac{32594.9}{1764} \approx 18.48
\label{eq:chisqstatcalc}
\end{equation}

Using the lower-tail critical values of chi-square distribution of the Engineering Statistics Handbook \cite{nist}, the critical value is approximated by 
\\
\begin{equation}
\chi^2_{0.95,9}\approx 3.33
\label{eq:criticalvalue}
\end{equation}
\\
with the corresponding p-value of
\\
\begin{equation}
p = P(\chi^2 < 18.48) = 1-0.03 = 0.97
\label{eq:pvalue}
\end{equation}
\\
using the p-Value Calculator for a Chi-Square Test \cite{freepvalue}. 

Consequently, as 
\\
\begin{equation}
\chi^2 \nleq \chi^2_{0.95,9}
\label{eq:criticalvalue}
\end{equation}
\\
or, put differently, the p-value is much larger than our $\alpha$, we can not reject $H_0$ and can not say that the new model makes more constant weights. Looking at the large initial difference in variances $s_2^2 \approx 3621.66 $ and $\sigma^2_1 = 1764$, this result is not too surprising. 




-formulate Hypotheses\\
-what test\\
-what decision rule\\
-what p value/critical values\\
-do calculations




we can use the Central Limit Theorem as proven by Hogg et al. \cite[Theorem~5.3.1]{hogg}. 
If we now want to model the weight distribution of our hammers, the best assumption is a normal distribution. The 1000 produced hammers can be interpreted as a random sample of the population of hammers with $n = 1000$. This $n$ is $>30$, so the central limit theorem can be applied here. 



\chapter{Workbook Assignment 5: Sufficient Statistics}	
Let $X_1,~X_2,~...,~X_n$ be the random sample of a positive random variable $X$ having the density function 
\begin{equation}
\begin{split}
f_X(x;~\theta) = \frac{1}{2!~\theta^3}x^2e^{-\frac{x}{\theta}}
\end{split}
\label{eq:suffpdf}
\end{equation}
with one parameter $\theta \in \mathbb{R}+$. Find an estimator for $\theta$ that is sufficient. 
\\
Larsen and Marx define Sufficient Statistics as follows: "Let $X_1 = k_1, . . . , X_n = k_n$ be a random sample of size $n$ from $p_X(k;~\theta)$. The statistic $\hat{\theta} = h(X_1, . . . , X_n)$ is sufficient for $\theta$ if the likelihood function, $L(\theta)$, factors into the product of the pdf for $\hat{\theta}$ and a constant that does not involve $\theta$ — that is, if
\begin{equation}
\begin{split}
L(\theta) 
= \prod_{i=1}^n p_X(k_i;\theta)
=p_{\hat{\theta}}(\theta_e;\theta)b(k_1,...,k_n)
\end{split}
\label{eq:SufficientLikelihood}
\end{equation}
A similar statement holds if the data consist of a random sample $Y_1~=~y_1,...,Y_n~=~ y_n$ drawn from a coninuous pdf $f_Y(y;\theta)$." \cite{larsen2005introduction}.
This can be constructed for the given density function \eqref{eq:suffpdf}. Firstly, to calculate the likelihood function, its definition \eqref{eq:LikelyhoodFunctionDefinition} is used for \eqref{eq:SufficientLikelihood}.
\begin{equation}
\begin{split}
L(\theta) 
&= \prod_{i=1}^n f_X(x_i;~\theta)\\
&= \prod_{i=1}^n \frac{1}{2!~\theta^3}x_i^2e^{-\frac{x_i}{\theta}}\\
\end{split}
\label{eq:SufficientLikelihood2}
\end{equation}
This can be simplified to 
\begin{equation}
\begin{split}
L(\theta) 
&= \frac{1}{2^n\theta^{3n}}\prod_{i=1}^n x_i^2e^{-\frac{x_i}{\theta}}\\
&= \frac{1}{2^n\theta^{3n}} e^{-\frac{1}{\theta}\sum_{i=1}^nx_i} \left(\prod_{i=1}^n x_i\right)^2\\
\end{split}
\label{eq:SufficientLikelihood3}
\end{equation}
Now
\begin{equation}
\begin{split}
p_{\hat{\theta}}(\theta_e;\theta) = \frac{1}{2^n\theta^{3n}} e^{-\frac{1}{\theta}\sum_{i=1}^nx_i}
\end{split}
\label{eq:SufficientLikelihood4}
\end{equation}
and
\begin{equation}
\begin{split}
b(k_1,...,k_n) = \left(\prod_{i=1}^n x_i\right)^2
\end{split}
\label{eq:SufficientLikelihood5}
\end{equation}
As \eqref{eq:SufficientLikelihood5} is not a function of $\theta$, $\sum_{i=1}^nx_i$ is a sufficient estimator for $\theta$.






\chapter{Workbook Assignment 6: Bayesian Estimates}	
Let $X1, X2, ... , X10$ be a random sample from a gamma distribution with $\alpha =3$ and $\beta =1/\theta$. Suppose we believe that $\theta$ follows a gamma-distribution with $\alpha =3$ and $\beta = 2$.

\section{Posterior Distribution}
To calculate the posterior distribution, the following Bayesian model can be formulated from the task assignment:
\begin{equation}
\begin{split}
X_i|\theta &\sim \Gamma\left(3,\frac{1}{\theta}\right)\\
\Theta &\sim \Gamma(3,2).
\end{split}
\label{eq:Bayesmodel}
\end{equation}
The Gamma distribution which both $X$ and $\Theta$ follow is given by Hogg et al. \cite[Equation~3.3.2]{hogg}:
\begin{equation}
\begin{split}
f(x) = \frac{x^{\alpha-1}e^{-\frac{x}{\beta}}}{\Gamma(\alpha)\beta^\alpha}\quad x>0.
\end{split}
\label{eq:GammaDist}
\end{equation}
Generally, the posterior pdf $k(\theta|X)$ is given by Hogg et al. as: \cite[Equation~11.1.5]{hogg} 
\begin{equation}
\begin{split}
k(\theta|x) = \frac{L(x|\theta)h(\theta)}{g_1(x)}
\end{split}
\label{eq:Posterior}
\end{equation}
where 
\begin{equation}
\begin{split}
g_1(x) = \int_{-\infty}^\infty L(x|\theta)h(\theta) ~d\theta.
\end{split}
\label{eq:g_1}
\end{equation}
Following Hogg et al. \cite[Chapter 11.1.1]{hogg} the simplified notation of 
\begin{equation}
\begin{split}
k(\theta|x) \propto L(x|\theta)h(\theta)
\end{split}
\label{eq:Posterior2}
\end{equation}
is sufficient to calculate the posterior distribution as $g_1(x)$ is a normalising factor in this context. The calculation of $g_1(x)$ can consequently be dropped. $L(x|\theta)$ is obtained by inserting the upper model of \eqref{eq:Bayesmodel} into \eqref{eq:GammaDist}:
\begin{equation}
\begin{split}
L(x|\theta) 
&= \prod_{i=1}^n \frac{x_i^{3-1}e^{-\frac{x_i}{\frac{1}{\theta}}}}{\Gamma(3)\left(\frac{1}{\theta}\right)^3} \\
&= \prod_{i=1}^n \frac{x_i^{2}\theta^3e^{-x_i\theta}}{\Gamma(3)}\\
&= \frac{x^{2n}\theta^{3n}e^{-\theta\sum_{i=1}^nx_i}}{\Gamma(3)^n}.
\end{split}
\label{eq:bayeslikelyhood}
\end{equation}
The prior pdf is given by the lower part of \eqref{eq:Bayesmodel}:
\begin{equation}
\begin{split}
h(\theta) = \frac{\theta^{3-1}e^{-\frac{\theta}{2}}}{\Gamma(3)2^3}
=\frac{\theta^{2}e^{-\frac{\theta}{2}}}{8\Gamma(3)} .
\end{split}
\label{eq:BayesPrior}
\end{equation}
Consequently, 
\begin{equation}
\begin{split}
k(\theta|x) 
\propto \frac{x^{2n}\theta^{3n}e^{-\theta\sum_{i=1}^nx_i}}{\Gamma(3)^n} \frac{\theta^{2}e^{-\frac{\theta}{2}}}{8\Gamma(3)},
\end{split}
\label{eq:calculatedposterior}
\end{equation}
assuming $0<\theta<\infty$. Dropping the constants results in 
\begin{equation}
\begin{split}
k(\theta|x) 
&\propto \theta^{3n}\theta^{2}e^{-\theta\sum_{i=1}^nx_i} e^{-\frac{\theta}{2}}\\
&\propto \theta^{3n+2}e^{-\theta(\frac{1}{2}+\sum_{i=1}^nx_i)}
\end{split}
\label{eq:calculatedposterior2}
\end{equation}
Looking at the form of \eqref{eq:calculatedposterior2}, it is apparent that it is a $\Gamma$ function. $\alpha$ and $\beta$ can now be calculated using \eqref{eq:calculatedposterior2}, \eqref{eq:GammaDist} and $n=10$ from the assignment.
\begin{equation}
\begin{split}
\alpha-1 &= 3n+2\\
\alpha &= 33
\end{split}
\label{eq:postalpha}
\end{equation}
\begin{equation}
\begin{split}
-\frac{\theta}{\beta} &= -\theta\left(\frac{1}{2}+\sum_{i=1}^nx_i\right)\\
\beta &= \frac{1}{\frac{1}{2}+\sum_{i=1}^nx_i}
\end{split}
\label{eq:postbeta}
\end{equation}
The posterior distribution is consequently 
\begin{equation}
\begin{split}
\Theta|X &\sim \Gamma\left(33,\frac{1}{\frac{1}{2}+\sum_{i=1}^nx_i}\right).
\end{split}
\label{eq:Posteriormodel}
\end{equation}

\section{Bayes Point Estimate by Square-Error Loss Function}
Assumed, $\bar{x} = 16.5$ is observed, what is the Bayes point estimate associated with the square-error loss function?
Following Theorem 5.8.1 \cite{larsen2005introduction}, the Bayes estimate for $\theta$ is the mean of the posterior if a square-error loss function is used. As the distribution is a Gamma-distribution, the mean or expected value is calculated by 
\begin{equation}
\begin{split}
E(X|\Theta) = \alpha\beta
\end{split}
\label{eq:expvalgamma}
\end{equation}
as proven in Chapter 3.3 \cite{hogg}. Using $\alpha = 33$ and 
\begin{equation}
\begin{split}
\beta &= \frac{1}{\frac{1}{2}+\sum_{i=1}^nx_i}\\
&=\frac{1}{\frac{1}{2}+n\bar{x}}\\
&=\frac{1}{\frac{1}{2}+10\cdot16.5}\\
&=\frac{1}{165.5},
\end{split}
\label{eq:postbeta}
\end{equation}
the mean computes to 
\begin{equation}
\begin{split}
E(X|\Theta) = \frac{33}{165.5} \approx 0.1994.
\end{split}
\label{eq:expvalgammacalc}
\end{equation}

\section{Point Estimate by Mode}
What is the Bayes point estimate using the mode of the posterior distribution?
As the mode of a distribution is simply its highest point, it is the maximum of the posterior distribution. Consequently, the maximum is found by deriving the posterior distribution with respect to $\theta$, equating it to 0 and solving it. The posterior distribution is
\begin{equation}
\begin{split}
f(\theta) = \frac{\theta^{32}e^{-\theta(0.5+\sum_{i=1}^n x_i)}}
{\Gamma(33)\beta^\alpha},
\end{split}
\label{eq:postdistinserted}
\end{equation}
where the denominator is constant and can be excluded. The derivative computes to
\begin{equation}
\begin{split}
f_\theta'(\theta) = 32\theta^{31}e^{-\theta\left(0.5+\sum_{i=1}^n x_i\right)}+e^{-\theta\left(0.5+\sum_{i=1}^n x_i\right)}\left(-0.5-\sum_{i=1}^n x_i\right)\theta^{32}
\end{split}
\label{eq:postdistderived}
\end{equation}
and results in the following roots if equated to 0:
\begin{equation}
\begin{split}
\theta_1 = 0 \qquad \theta_2 = \frac{32}{0.5+\sum_{i=1}^n x_i}.
\end{split}
\label{eq:roots}
\end{equation}
$\theta_1 = 0$ is no solution for a gamma distribution. Consequently, $\theta_2$ is the only possible solution. Inserting $\bar{x} = 16.5$ from previous section results in 
\begin{equation}
\begin{split}
\theta_2 = \frac{32}{0.5+10\cdot16.5} = \frac{32}{165.5} \approx 0.1934.
\end{split}
\label{eq:rootsN}
\end{equation}
